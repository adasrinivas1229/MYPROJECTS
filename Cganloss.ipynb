{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cganloss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adasrinivas1229/MYPROJECTS/blob/master/Cganloss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9JzMit2RlaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import os\n",
        "from google.colab import drive\n",
        "from glob import glob\n",
        "from keras import Input, Model\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.layers import Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import Add, Conv2DTranspose, ZeroPadding2D, LeakyReLU\n",
        "from keras.optimizers import Adam\n",
        "from imageio import imread\n",
        "from skimage.transform import resize\n",
        "#from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP9_MfsSSXut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def residual_block(x):\n",
        "\n",
        "    \"\"\"\n",
        "    Residual block\n",
        "    \"\"\"\n",
        "    \n",
        "    res = Conv2D(filters = 128, kernel_size = 3, strides = 1, padding = \"same\")(x)\n",
        "    res = BatchNormalization(axis = 3, momentum = 0.8, epsilon = 1e-5)(res)\n",
        "    res = Activation('relu')(res)\n",
        "    res = Conv2D(filters = 128, kernel_size = 3, strides = 1, padding = \"same\")(res)\n",
        "    res = BatchNormalization(axis = 3, momentum = 0.8, epsilon = 1e-5)(res)\n",
        "    \n",
        "    return Add()([res, x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-yVb-WhDKjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Layer, InputSpec\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras import backend as K\n",
        "#from tensorflow import backend as tf\n",
        "class InstanceNormalization(Layer):\n",
        "    \"\"\"Instance normalization layer.\n",
        "    Normalize the activations of the previous layer at each step,\n",
        "    i.e. applies a transformation that maintains the mean activation\n",
        "    close to 0 and the activation standard deviation close to 1.\n",
        "    # Arguments\n",
        "        axis: Integer, the axis that should be normalized\n",
        "            (typically the features axis).\n",
        "            For instance, after a `Conv2D` layer with\n",
        "            `data_format=\"channels_first\"`,\n",
        "            set `axis=1` in `InstanceNormalization`.\n",
        "            Setting `axis=None` will normalize all values in each\n",
        "            instance of the batch.\n",
        "            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.\n",
        "        epsilon: Small float added to variance to avoid dividing by zero.\n",
        "        center: If True, add offset of `beta` to normalized tensor.\n",
        "            If False, `beta` is ignored.\n",
        "        scale: If True, multiply by `gamma`.\n",
        "            If False, `gamma` is not used.\n",
        "            When the next layer is linear (also e.g. `nn.relu`),\n",
        "            this can be disabled since the scaling\n",
        "            will be done by the next layer.\n",
        "        beta_initializer: Initializer for the beta weight.\n",
        "        gamma_initializer: Initializer for the gamma weight.\n",
        "        beta_regularizer: Optional regularizer for the beta weight.\n",
        "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
        "        beta_constraint: Optional constraint for the beta weight.\n",
        "        gamma_constraint: Optional constraint for the gamma weight.\n",
        "    # Input shape\n",
        "        Arbitrary. Use the keyword argument `input_shape`\n",
        "        (tuple of integers, does not include the samples axis)\n",
        "        when using this layer as the first layer in a Sequential model.\n",
        "    # Output shape\n",
        "        Same shape as input.\n",
        "    # References\n",
        "        - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n",
        "        - [Instance Normalization: The Missing Ingredient for Fast Stylization](\n",
        "        https://arxiv.org/abs/1607.08022)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 axis=None,\n",
        "                 epsilon=1e-3,\n",
        "                 center=True,\n",
        "                 scale=True,\n",
        "                 beta_initializer='zeros',\n",
        "                 gamma_initializer='ones',\n",
        "                 beta_regularizer=None,\n",
        "                 gamma_regularizer=None,\n",
        "                 beta_constraint=None,\n",
        "                 gamma_constraint=None,\n",
        "                 **kwargs):\n",
        "        super(InstanceNormalization, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.axis = axis\n",
        "        self.epsilon = epsilon\n",
        "        self.center = center\n",
        "        self.scale = scale\n",
        "        self.beta_initializer = initializers.get(beta_initializer)\n",
        "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
        "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
        "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
        "        self.beta_constraint = constraints.get(beta_constraint)\n",
        "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        ndim = len(input_shape)\n",
        "        if self.axis == 0:\n",
        "            raise ValueError('Axis cannot be zero')\n",
        "\n",
        "        if (self.axis is not None) and (ndim == 2):\n",
        "            raise ValueError('Cannot specify axis for rank 1 tensor')\n",
        "\n",
        "        self.input_spec = InputSpec(ndim=ndim)\n",
        "\n",
        "        if self.axis is None:\n",
        "            shape = (1,)\n",
        "        else:\n",
        "            shape = (input_shape[self.axis],)\n",
        "\n",
        "        if self.scale:\n",
        "            self.gamma = self.add_weight(shape=shape,\n",
        "                                         name='gamma',\n",
        "                                         initializer=self.gamma_initializer,\n",
        "                                         regularizer=self.gamma_regularizer,\n",
        "                                         constraint=self.gamma_constraint)\n",
        "        else:\n",
        "            self.gamma = None\n",
        "        if self.center:\n",
        "            self.beta = self.add_weight(shape=shape,\n",
        "                                        name='beta',\n",
        "                                        initializer=self.beta_initializer,\n",
        "                                        regularizer=self.beta_regularizer,\n",
        "                                        constraint=self.beta_constraint)\n",
        "        else:\n",
        "            self.beta = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        input_shape = K.int_shape(inputs)\n",
        "        reduction_axes = list(range(0, len(input_shape)))\n",
        "\n",
        "        if self.axis is not None:\n",
        "            del reduction_axes[self.axis]\n",
        "\n",
        "        del reduction_axes[0]\n",
        "\n",
        "        mean = K.mean(inputs, reduction_axes, keepdims=True)\n",
        "        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n",
        "        normed = (inputs - mean) / stddev\n",
        "\n",
        "        broadcast_shape = [1] * len(input_shape)\n",
        "        if self.axis is not None:\n",
        "            broadcast_shape[self.axis] = input_shape[self.axis]\n",
        "\n",
        "        if self.scale:\n",
        "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
        "            normed = normed * broadcast_gamma\n",
        "        if self.center:\n",
        "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
        "            normed = normed + broadcast_beta\n",
        "        return normed\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'axis': self.axis,\n",
        "            'epsilon': self.epsilon,\n",
        "            'center': self.center,\n",
        "            'scale': self.scale,\n",
        "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
        "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
        "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
        "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
        "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
        "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
        "        }\n",
        "        base_config = super(InstanceNormalization, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hcZ3EV7Sxu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator():\n",
        "\n",
        "    \"\"\"\n",
        "    Creating a generator network with the hyperparameters defined below\n",
        "    \"\"\"\n",
        "    \n",
        "    input_shape = (128, 128, 3)\n",
        "    residual_blocks = 6\n",
        "    input_layer = Input(shape = input_shape)\n",
        "    \n",
        "    \n",
        "    ## 1st Convolutional Block\n",
        "    x = Conv2D(filters = 32, kernel_size = 7, strides = 1, padding = \"same\")(input_layer)\n",
        "    x = InstanceNormalization(axis = 1)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    \n",
        "    ## 2nd Convolutional Block\n",
        "    x = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = \"same\")(x)\n",
        "    x = InstanceNormalization(axis =1)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    \n",
        "    ## 3rd Convolutional Block\n",
        "    x = Conv2D(filters = 128, kernel_size = 3, strides = 2, padding = \"same\")(x)\n",
        "    x = InstanceNormalization(axis =1)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    \n",
        "    \n",
        "    ## Residual blocks\n",
        "    for _ in range(residual_blocks):\n",
        "      x = residual_block(x)\n",
        "      \n",
        "    \n",
        "    ## 1st Upsampling Block\n",
        "    x = Conv2DTranspose(filters = 64, kernel_size = 3, strides = 2, padding = \"same\", \n",
        "                        use_bias = False)(x)\n",
        "    x = InstanceNormalization(axis =1)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    \n",
        "    ## 2nd Upsampling Block\n",
        "    x = Conv2DTranspose(filters = 32, kernel_size = 3, strides = 2, padding = \"same\", \n",
        "                        use_bias = False)(x)\n",
        "    x = InstanceNormalization(axis =1)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    \n",
        "    ## Last Convolutional Layer\n",
        "    x = Conv2D(filters = 3, kernel_size = 7, strides = 1, padding = \"same\")(x)\n",
        "    output = Activation(\"tanh\")(x)\n",
        "    \n",
        "    \n",
        "    model = Model(inputs = [input_layer], outputs = [output])\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH9uOTpxS2yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator():\n",
        "\n",
        "    \"\"\"\n",
        "    Create a discriminator network using the hyperparameters defined below\n",
        "    \"\"\"\n",
        "    input_shape = (128, 128, 3)\n",
        "    hidden_layers = 3\n",
        "    input_layer = Input(shape = input_shape) \n",
        "    x = ZeroPadding2D(padding = (1, 1))(input_layer)\n",
        "    \n",
        "    \n",
        "    ## 1st Convolutional Block\n",
        "    x = Conv2D(filters = 64, kernel_size = 4, strides = 2, padding = \"valid\")(x)\n",
        "    x = LeakyReLU(alpha = 0.2)(x)\n",
        "    x = ZeroPadding2D(padding = (1, 1))(x)\n",
        "    \n",
        "    ## 3 Hidden Convolutional Blocks\n",
        "    for i in range(1, hidden_layers + 1):\n",
        "      x = Conv2D(filters = 2 ** i * 64, kernel_size = 4, strides = 2, padding = \"valid\")(x) #128,256,512\n",
        "      x = InstanceNormalization(axis =1)(x)\n",
        "      x = LeakyReLU(alpha = 0.2)(x)\n",
        "      x = ZeroPadding2D(padding = (1, 1))(x)\n",
        "      \n",
        "    \n",
        "    ## Last Convolutional Layer\n",
        "    output = Conv2D(filters = 1, kernel_size = 4, strides = 1, activation = \"sigmoid\")(x)\n",
        "    \n",
        "    model = Model(inputs = [input_layer], outputs = [output])\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJMUoJTGTXZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_images(data_dir):\n",
        "  \n",
        "  imagesA = glob(data_dir + '/trainA/*.*')\n",
        "  imagesB = glob(data_dir + '/trainB/*.*')\n",
        "  \n",
        "  allImagesA = []\n",
        "  allImagesB = []\n",
        "  \n",
        "  for index, filename in enumerate(imagesA):\n",
        "    imgA = imread(filename, pilmode = \"RGB\")\n",
        "    imgB = imread(imagesB[index], pilmode = \"RGB\")\n",
        "    \n",
        "    imgA = resize(imgA, (128, 128))\n",
        "    imgB = resize(imgB, (128, 128))\n",
        "    \n",
        "    if np.random.random() > 0.5:\n",
        "      imgA = np.fliplr(imgA)\n",
        "      imgB = np.fliplr(imgB)\n",
        "      \n",
        "    \n",
        "    allImagesA.append(imgA)\n",
        "    allImagesB.append(imgB)\n",
        "    \n",
        "  \n",
        "  ## Normalize images\n",
        "  allImagesA = np.array(allImagesA) / 127.5 - 1.\n",
        "  allImagesB = np.array(allImagesB) / 127.5 - 1.\n",
        "  \n",
        "  return allImagesA, allImagesB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULDHzx8HTfhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_test_batch(data_dir, batch_size):\n",
        "  \n",
        "  imagesA = glob(data_dir + '/testA/*.*')\n",
        "  imagesB = glob(data_dir + '/testB/*.*')\n",
        "  \n",
        "  imagesA = np.random.choice(a = imagesA, size = batch_size)\n",
        "  imagesB = np.random.choice(a = imagesB, size = batch_size)\n",
        "  \n",
        "  allA = []\n",
        "  allB = []\n",
        "  \n",
        "  for i in range(len(imagesA)):\n",
        "    ## Load and resize images\n",
        "    imgA = resize(imread(imagesA[i], pilmode = 'RGB').astype(np.float32), (128, 128))\n",
        "    imgB = resize(imread(imagesB[i], pilmode = 'RGB').astype(np.float32), (128, 128))\n",
        "    \n",
        "    allA.append(imgA)\n",
        "    allB.append(imgB)\n",
        "  #normalize images as -1 ,1\n",
        "  return np.array(allA) / 127.5 - 1.0, np.array(allB) / 127.5 - 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AALvnAHqURSz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a687ed1-ab4d-40d8-fed3-53ce2e86d2b6"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "data_dir = 'drive/My Drive/Colab Notebooks/monet2photo/'\n",
        "\n",
        "batch_size = 1\n",
        "epochs = 10\n",
        "mode = 'train'\n",
        "\n",
        "if mode == 'train':\n",
        "  \n",
        "  #Load dataset\n",
        "  imagesA, imagesB = load_images(data_dir = data_dir)\n",
        "  \n",
        "  #Define the common optimizer\n",
        "  common_optimizer = Adam(0.002, 0.5)\n",
        "  ## Build and compile discriminator networks\n",
        "  discriminatorA = build_discriminator()\n",
        "  \n",
        "  discriminatorB = build_discriminator()\n",
        "  \n",
        "  #Build generator networks\n",
        "  generatorA_to_B = build_generator()\n",
        "  generatorB_to_A = build_generator()\n",
        "  #Create an adversarial network\n",
        "  inputA = Input(shape = (128, 128, 3))\n",
        "  inputB = Input(shape = (128, 128, 3))\n",
        "    \n",
        "  \n",
        "  # --> Generated images using both of the generator networks\n",
        "  generatedB = generatorA_to_B(inputA)\n",
        "  generatedA = generatorB_to_A(inputB)\n",
        "  print(generatedB)\n",
        "  \n",
        "  #--> Reconstruct the images back to the original ones\n",
        "  reconstructedA = generatorB_to_A(generatedB)\n",
        "  reconstructedB = generatorA_to_B(generatedA)\n",
        "  print(reconstructedA)\n",
        "  \n",
        " \n",
        "  generatedA_Id = generatorB_to_A(inputA)\n",
        "  generatedB_Id = generatorA_to_B(inputB)\n",
        "  #print(generatedA_Id)\n",
        " \n",
        "  #Make both of the discriminator networks non-trainable\n",
        "  discriminatorA.trainable = False\n",
        "  discriminatorB.trainable = False\n",
        "    \n",
        "  probsA = discriminatorA(generatedA)\n",
        "  probsB = discriminatorB(generatedB)\n",
        "  \n",
        "\n",
        "  discriminatorA.compile(loss = 'mse', \n",
        "                           optimizer = common_optimizer,\n",
        "                           metrics = ['accuracy'])\n",
        "  discriminatorB.compile(loss = 'mse',\n",
        "                           optimizer = common_optimizer,\n",
        "                           metrics = ['accuracy'])\n",
        "  \n",
        "  \n",
        "  adversarial_model = Model(inputs = [inputA, inputB],\n",
        "                              outputs = [probsA, probsB, \n",
        "                                         reconstructedA, reconstructedB])\n",
        "  \"\"\"\n",
        "  adversarial_model.compile(loss = ['mse', 'mse', 'mae', 'mae', 'mae', 'mae'],\n",
        "                              loss_weights = [1, 1, 10.0, 10.0, 1.0, 1.0],\n",
        "                            optimizer = common_optimizer)\n",
        "  \"\"\"   \n",
        "  adversarial_model.compile(loss = ['mse', 'mse', 'mae', 'mae'],\n",
        "                              loss_weights = [1, 1, 10.0, 10.0],\n",
        "                              optimizer = common_optimizer)\n",
        "  real_labels = np.ones((batch_size, 7, 7, 1))\n",
        "  fake_labels = np.zeros((batch_size, 7, 7, 1))\n",
        "    \n",
        "  for epoch in range(epochs):\n",
        "    print(\"Epoch: {}\".format(epoch))\n",
        "    \n",
        "    num_batches = int(min(imagesA.shape[0], imagesB.shape[0]) / batch_size)\n",
        "    print(\"Number of batches: {}\".format(num_batches))\n",
        "\n",
        "  for index in range(num_batches):\n",
        "    print(\"Batch: {}\".format(index))\n",
        "        \n",
        "    ## Sample images\n",
        "    batchA = imagesA[index * batch_size: (index + 1) * batch_size]\n",
        "    batchB = imagesB[index * batch_size: (index + 1) * batch_size]\n",
        "        \n",
        "    ## Translate images to opposite domain\n",
        "    generatedB = generatorA_to_B.predict(batchA)\n",
        "    generatedA = generatorB_to_A.predict(batchB)\n",
        "        \n",
        "    ## Train the discriminator A on real and fake images\n",
        "    D_A_Loss1 = discriminatorA.train_on_batch(batchA, real_labels) #x-batchA\n",
        "    D_A_Loss2 = discriminatorA.train_on_batch(generatedA, fake_labels)#y-generatedA\n",
        "        \n",
        "    ## Train the discriminator B on real and fake images\n",
        "    D_B_Loss1 = discriminatorB.train_on_batch(batchB, real_labels)#x^\n",
        "    D_B_Loss2 = discriminatorB.train_on_batch(generatedB, fake_labels)#generatedB-y^\n",
        "        \n",
        "    ## Calculate the total discriminator loss\n",
        "    D_loss = 0.5 * np.add(0.5 * np.add(D_A_Loss1, D_A_Loss2), \n",
        "                              0.5 * np.add(D_B_Loss1, D_B_Loss2))\n",
        "        \n",
        "    print(\"D_Loss: {}\".format(D_loss))\n",
        "        \n",
        "        \n",
        "    #Train the generator networks   \n",
        "    G_loss = adversarial_model.train_on_batch([batchA, batchB],\n",
        "                                                  [real_labels, real_labels,\n",
        "                                                   batchA, batchB])\n",
        "        \n",
        "    print(\"G_Loss: {}\".format(G_loss))\n",
        "    \n",
        "    \n",
        "      \n",
        "    # Sample and save images after every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "      # Get a batch of test data\n",
        "      batchA, batchB = load_test_batch(data_dir = data_dir, batch_size = 2)\n",
        "        \n",
        "      # Generate images\n",
        "      generatedB = generatorA_to_B.predict(batchA)\n",
        "      generatedA = generatorB_to_A.predict(batchB)\n",
        "      print(generatedB)\n",
        "      # Get reconstructed images\n",
        "      recons_A = generatorB_to_A.predict(generatedB)\n",
        "      recons_B = generatorA_to_B.predict(generatedA)\n",
        "      print(recons_A)\n",
        " \n",
        "\n",
        "\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Tensor(\"model_49/activation_252/Tanh:0\", shape=(None, 128, None, 3), dtype=float32)\n",
            "Tensor(\"model_50_1/activation_264/Tanh:0\", shape=(None, 128, None, 3), dtype=float32)\n",
            "Epoch: 0\n",
            "Number of batches: 100\n",
            "Epoch: 1\n",
            "Number of batches: 100\n",
            "Epoch: 2\n",
            "Number of batches: 100\n",
            "Epoch: 3\n",
            "Number of batches: 100\n",
            "Epoch: 4\n",
            "Number of batches: 100\n",
            "Epoch: 5\n",
            "Number of batches: 100\n",
            "Epoch: 6\n",
            "Number of batches: 100\n",
            "Epoch: 7\n",
            "Number of batches: 100\n",
            "Epoch: 8\n",
            "Number of batches: 100\n",
            "Epoch: 9\n",
            "Number of batches: 100\n",
            "Batch: 0\n",
            "D_Loss: [0.44610944 0.43367347]\n",
            "G_Loss: [21.364355, 0.0926803, 0.5561995, 1.0161757, 1.0553718]\n",
            "Batch: 1\n",
            "D_Loss: [0.30693766 0.5408163 ]\n",
            "G_Loss: [4.4461637, 0.2501974, 0.681328, 0.2125904, 0.13887341]\n",
            "Batch: 2\n",
            "D_Loss: [0.31844535 0.5       ]\n",
            "G_Loss: [1.0089115, 0.21001205, 0.6918007, 0.004920561, 0.005789315]\n",
            "Batch: 3\n",
            "D_Loss: [0.31845784 0.5       ]\n",
            "G_Loss: [0.96485627, 0.20984161, 0.69169456, 0.003235056, 0.0030969586]\n",
            "Batch: 4\n",
            "D_Loss: [0.31849527 0.5       ]\n",
            "G_Loss: [0.9951565, 0.20984887, 0.6916683, 0.0039711324, 0.0053928]\n",
            "Batch: 5\n",
            "D_Loss: [0.31848723 0.5       ]\n",
            "G_Loss: [0.9827952, 0.20985107, 0.6916582, 0.0038308846, 0.0042977044]\n",
            "Batch: 6\n",
            "D_Loss: [0.31846446 0.5       ]\n",
            "G_Loss: [0.9740427, 0.20985056, 0.6916518, 0.004434369, 0.0028196634]\n",
            "Batch: 7\n",
            "D_Loss: [0.3184521 0.5      ]\n",
            "G_Loss: [0.97990894, 0.20984836, 0.6916465, 0.0055425046, 0.0022989004]\n",
            "Batch: 8\n",
            "D_Loss: [0.318469 0.5     ]\n",
            "G_Loss: [0.971062, 0.20984414, 0.6916404, 0.0039269375, 0.0030308147]\n",
            "Batch: 9\n",
            "D_Loss: [0.31845003 0.5       ]\n",
            "G_Loss: [0.95618165, 0.20983632, 0.69163156, 0.0029253755, 0.0025459998]\n",
            "Batch: 10\n",
            "D_Loss: [0.3184682 0.5      ]\n",
            "G_Loss: [0.9740783, 0.20982791, 0.6916131, 0.0043183723, 0.002945357]\n",
            "Batch: 11\n",
            "D_Loss: [0.31849408 0.5       ]\n",
            "G_Loss: [0.96596664, 0.20981717, 0.6915434, 0.003816039, 0.0026445687]\n",
            "Batch: 12\n",
            "D_Loss: [0.31845552 0.5       ]\n",
            "G_Loss: [0.9728311, 0.20980625, 0.69121075, 0.004990558, 0.0021908453]\n",
            "Batch: 13\n",
            "D_Loss: [0.31844628 0.5       ]\n",
            "G_Loss: [0.97920704, 0.20978898, 0.69129086, 0.0052434313, 0.0025692882]\n",
            "Batch: 14\n",
            "D_Loss: [0.31848046 0.5       ]\n",
            "G_Loss: [0.98886865, 0.2097507, 0.68976796, 0.0042120162, 0.0047229817]\n",
            "Batch: 15\n",
            "D_Loss: [0.318408 0.5     ]\n",
            "G_Loss: [0.9664049, 0.20967713, 0.69161093, 0.0039575454, 0.002554142]\n",
            "Batch: 16\n",
            "D_Loss: [0.31850463 0.5       ]\n",
            "G_Loss: [0.97314197, 0.20953631, 0.6916021, 0.0031419913, 0.0040583625]\n",
            "Batch: 17\n",
            "D_Loss: [0.31849825 0.5       ]\n",
            "G_Loss: [0.9707026, 0.20923017, 0.6915863, 0.0025227782, 0.0044658286]\n",
            "Batch: 18\n",
            "D_Loss: [0.3184504 0.5      ]\n",
            "G_Loss: [0.962532, 0.20852265, 0.6915729, 0.0031681191, 0.003075522]\n",
            "Batch: 19\n",
            "D_Loss: [0.31851375 0.5       ]\n",
            "G_Loss: [0.97190285, 0.20939806, 0.6915717, 0.003674879, 0.0034184325]\n",
            "Batch: 20\n",
            "D_Loss: [0.3184329 0.5      ]\n",
            "G_Loss: [0.96026886, 0.20929153, 0.6915802, 0.0036634246, 0.002276287]\n",
            "Batch: 21\n",
            "D_Loss: [0.31849504 0.5       ]\n",
            "G_Loss: [0.9616166, 0.20858726, 0.69159377, 0.0032611846, 0.0028823693]\n",
            "Batch: 22\n",
            "D_Loss: [0.3184651 0.5      ]\n",
            "G_Loss: [0.9792791, 0.2096684, 0.6916077, 0.004359357, 0.0034409382]\n",
            "Batch: 23\n",
            "D_Loss: [0.31847042 0.5       ]\n",
            "G_Loss: [0.94242674, 0.20971051, 0.691612, 0.0036471342, 0.00046328708]\n",
            "Batch: 24\n",
            "D_Loss: [0.31847823 0.5       ]\n",
            "G_Loss: [0.9839111, 0.2096914, 0.69161844, 0.0054408987, 0.0028192275]\n",
            "Batch: 25\n",
            "D_Loss: [0.31849363 0.5       ]\n",
            "G_Loss: [0.96961296, 0.2095953, 0.6916208, 0.0040515717, 0.002788107]\n",
            "Batch: 26\n",
            "D_Loss: [0.31857473 0.5       ]\n",
            "G_Loss: [0.9707468, 0.2094202, 0.6916199, 0.0030658252, 0.0039048519]\n",
            "Batch: 27\n",
            "D_Loss: [0.31876284 0.5       ]\n",
            "G_Loss: [0.96344244, 0.20914064, 0.6916206, 0.0036387842, 0.002629334]\n",
            "Batch: 28\n",
            "D_Loss: [0.31845033 0.5       ]\n",
            "G_Loss: [0.97965497, 0.2085087, 0.6916213, 0.0039171744, 0.0040353173]\n",
            "Batch: 29\n",
            "D_Loss: [0.31849244 0.5       ]\n",
            "G_Loss: [0.94952273, 0.20978774, 0.6916203, 0.0031017489, 0.001709721]\n",
            "Batch: 30\n",
            "D_Loss: [0.318473 0.5     ]\n",
            "G_Loss: [0.96200204, 0.20984301, 0.69161505, 0.0036693707, 0.0023850317]\n",
            "Batch: 31\n",
            "D_Loss: [0.31844163 0.5       ]\n",
            "G_Loss: [0.975611, 0.2098498, 0.69161165, 0.0039675534, 0.0034473985]\n",
            "Batch: 32\n",
            "D_Loss: [0.31841272 0.5       ]\n",
            "G_Loss: [0.9750203, 0.20984873, 0.6916046, 0.0041207904, 0.0032359]\n",
            "Batch: 33\n",
            "D_Loss: [0.3184482 0.5      ]\n",
            "G_Loss: [0.9641363, 0.20984088, 0.69159424, 0.002999138, 0.0032709767]\n",
            "Batch: 34\n",
            "D_Loss: [0.3184451 0.5      ]\n",
            "G_Loss: [0.9645061, 0.20983161, 0.6915919, 0.003783025, 0.0025252341]\n",
            "Batch: 35\n",
            "D_Loss: [0.3184399 0.5      ]\n",
            "G_Loss: [0.96661794, 0.20981851, 0.69159496, 0.004029782, 0.0024906695]\n",
            "Batch: 36\n",
            "D_Loss: [0.31841737 0.5       ]\n",
            "G_Loss: [0.9703885, 0.20980461, 0.6915972, 0.0038108344, 0.0030878284]\n",
            "Batch: 37\n",
            "D_Loss: [0.3183895 0.5      ]\n",
            "G_Loss: [0.9550209, 0.20978326, 0.6915924, 0.0035733443, 0.0017911778]\n",
            "Batch: 38\n",
            "D_Loss: [0.31846508 0.5       ]\n",
            "G_Loss: [0.9749758, 0.20980264, 0.69158864, 0.0030035297, 0.004354922]\n",
            "Batch: 39\n",
            "D_Loss: [0.31848183 0.5       ]\n",
            "G_Loss: [0.9728483, 0.20980734, 0.6915715, 0.0041870265, 0.0029599206]\n",
            "Batch: 40\n",
            "D_Loss: [0.3184762 0.5      ]\n",
            "G_Loss: [0.96956235, 0.20978542, 0.69155633, 0.0037096026, 0.00311246]\n",
            "Batch: 41\n",
            "D_Loss: [0.31849492 0.5       ]\n",
            "G_Loss: [0.9917402, 0.20975372, 0.69155526, 0.0038121797, 0.0052309395]\n",
            "Batch: 42\n",
            "D_Loss: [0.31846526 0.5       ]\n",
            "G_Loss: [0.98483074, 0.20975368, 0.69152904, 0.0046708127, 0.003683988]\n",
            "Batch: 43\n",
            "D_Loss: [0.3184839 0.5      ]\n",
            "G_Loss: [0.974452, 0.20970611, 0.69153476, 0.005160515, 0.002160595]\n",
            "Batch: 44\n",
            "D_Loss: [0.31852448 0.5       ]\n",
            "G_Loss: [0.9843786, 0.20961073, 0.6915618, 0.003470603, 0.00485]\n",
            "Batch: 45\n",
            "D_Loss: [0.31871367 0.5       ]\n",
            "G_Loss: [0.97030044, 0.20972383, 0.6915313, 0.0025306458, 0.004373883]\n",
            "Batch: 46\n",
            "D_Loss: [0.31863904 0.5       ]\n",
            "G_Loss: [0.95921135, 0.20973712, 0.6915291, 0.0036270102, 0.0021675078]\n",
            "Batch: 47\n",
            "D_Loss: [0.31869978 0.5       ]\n",
            "G_Loss: [0.97304, 0.20967071, 0.6915514, 0.0044147233, 0.002767065]\n",
            "Batch: 48\n",
            "D_Loss: [0.31867677 0.5       ]\n",
            "G_Loss: [0.95519954, 0.2094707, 0.69156295, 0.002814176, 0.0026024156]\n",
            "Batch: 49\n",
            "D_Loss: [0.31879354 0.5       ]\n",
            "G_Loss: [0.95881367, 0.20895055, 0.6915536, 0.0020819248, 0.003749028]\n",
            "Batch: 50\n",
            "D_Loss: [0.3187095 0.5      ]\n",
            "G_Loss: [0.9378284, 0.20933372, 0.69152737, 0.002245969, 0.0014507672]\n",
            "Batch: 51\n",
            "D_Loss: [0.3186522 0.5      ]\n",
            "G_Loss: [0.9545528, 0.20909508, 0.6915482, 0.0018525403, 0.00353841]\n",
            "Batch: 52\n",
            "D_Loss: [0.31851828 0.5       ]\n",
            "G_Loss: [0.97542953, 0.2096407, 0.69153994, 0.0039035655, 0.0035213241]\n",
            "Batch: 53\n",
            "D_Loss: [0.31850708 0.5       ]\n",
            "G_Loss: [0.9767251, 0.20961018, 0.691508, 0.0042874385, 0.0032732515]\n",
            "Batch: 54\n",
            "D_Loss: [0.318559 0.5     ]\n",
            "G_Loss: [0.94821775, 0.20930928, 0.69148666, 0.0018359809, 0.002906203]\n",
            "Batch: 55\n",
            "D_Loss: [0.3188428 0.5      ]\n",
            "G_Loss: [0.9342128, 0.20890743, 0.69151086, 0.0021963492, 0.0011831071]\n",
            "Batch: 56\n",
            "D_Loss: [0.31862986 0.5       ]\n",
            "G_Loss: [0.95230615, 0.20961265, 0.69156146, 0.0031514047, 0.0019617982]\n",
            "Batch: 57\n",
            "D_Loss: [0.3185925 0.5      ]\n",
            "G_Loss: [0.9600527, 0.20960839, 0.6915773, 0.0043445183, 0.0015421875]\n",
            "Batch: 58\n",
            "D_Loss: [0.31861237 0.5       ]\n",
            "G_Loss: [0.98562044, 0.2093458, 0.6915805, 0.0036775004, 0.0047919163]\n",
            "Batch: 59\n",
            "D_Loss: [0.31902224 0.5       ]\n",
            "G_Loss: [0.9621514, 0.20794556, 0.69155586, 0.0042620073, 0.002002991]\n",
            "Batch: 60\n",
            "D_Loss: [0.31838873 0.5       ]\n",
            "G_Loss: [0.96347123, 0.20969224, 0.6915441, 0.004038363, 0.002185126]\n",
            "Batch: 61\n",
            "D_Loss: [0.3184282 0.5      ]\n",
            "G_Loss: [0.9509324, 0.20979008, 0.6915327, 0.004060543, 0.000900425]\n",
            "Batch: 62\n",
            "D_Loss: [0.31845075 0.5       ]\n",
            "G_Loss: [0.9742027, 0.20979877, 0.6915644, 0.0041367584, 0.0031471984]\n",
            "Batch: 63\n",
            "D_Loss: [0.31848907 0.5       ]\n",
            "G_Loss: [0.9617046, 0.20978071, 0.69156617, 0.0039796894, 0.0020560848]\n",
            "Batch: 64\n",
            "D_Loss: [0.3184551 0.5      ]\n",
            "G_Loss: [0.96025294, 0.20973092, 0.6915612, 0.0024472908, 0.0034487878]\n",
            "Batch: 65\n",
            "D_Loss: [0.31848913 0.5       ]\n",
            "G_Loss: [0.97800994, 0.20973414, 0.6915316, 0.0041061365, 0.003568283]\n",
            "Batch: 66\n",
            "D_Loss: [0.31852502 0.5       ]\n",
            "G_Loss: [0.97336966, 0.20967549, 0.6914785, 0.0024383767, 0.0047831917]\n",
            "Batch: 67\n",
            "D_Loss: [0.3185497 0.5      ]\n",
            "G_Loss: [0.95609, 0.20967464, 0.69137317, 0.0034776675, 0.0020265463]\n",
            "Batch: 68\n",
            "D_Loss: [0.31847626 0.5       ]\n",
            "G_Loss: [0.96532315, 0.20969257, 0.6915019, 0.0032640921, 0.0031487786]\n",
            "Batch: 69\n",
            "D_Loss: [0.3185831 0.5      ]\n",
            "G_Loss: [0.96575737, 0.20961647, 0.69151133, 0.002597182, 0.0038657726]\n",
            "Batch: 70\n",
            "D_Loss: [0.3186428 0.5      ]\n",
            "G_Loss: [0.97319704, 0.20946783, 0.6914876, 0.0032867766, 0.003937384]\n",
            "Batch: 71\n",
            "D_Loss: [0.31862098 0.5       ]\n",
            "G_Loss: [0.9571139, 0.20926952, 0.6914184, 0.0033797694, 0.002262833]\n",
            "Batch: 72\n",
            "D_Loss: [0.31878832 0.5       ]\n",
            "G_Loss: [0.9692508, 0.20939481, 0.69148594, 0.003577833, 0.003259169]\n",
            "Batch: 73\n",
            "D_Loss: [0.3188025 0.5      ]\n",
            "G_Loss: [0.9632549, 0.20910074, 0.6915013, 0.0033119346, 0.002953354]\n",
            "Batch: 74\n",
            "D_Loss: [0.31909162 0.5       ]\n",
            "G_Loss: [0.977423, 0.20799686, 0.69149727, 0.0030259592, 0.0047669276]\n",
            "Batch: 75\n",
            "D_Loss: [0.31883818 0.5       ]\n",
            "G_Loss: [0.94330114, 0.20925961, 0.6914174, 0.0017264909, 0.0025359208]\n",
            "Batch: 76\n",
            "D_Loss: [0.3188091 0.5      ]\n",
            "G_Loss: [0.9614694, 0.20930842, 0.6914704, 0.0031992113, 0.0028698526]\n",
            "Batch: 77\n",
            "D_Loss: [0.31883675 0.5       ]\n",
            "G_Loss: [0.9577458, 0.20897996, 0.6914802, 0.00250311, 0.003225454]\n",
            "Batch: 78\n",
            "D_Loss: [0.3192478 0.5      ]\n",
            "G_Loss: [0.9693741, 0.20756516, 0.69147295, 0.004031476, 0.0030021304]\n",
            "Batch: 79\n",
            "D_Loss: [0.3185573 0.5      ]\n",
            "G_Loss: [0.9617553, 0.20956251, 0.69145745, 0.0031804894, 0.0028930418]\n",
            "Batch: 80\n",
            "D_Loss: [0.31851023 0.5       ]\n",
            "G_Loss: [0.9742749, 0.20969062, 0.6915013, 0.0048251906, 0.0024830967]\n",
            "Batch: 81\n",
            "D_Loss: [0.31844825 0.5       ]\n",
            "G_Loss: [0.96963084, 0.20968764, 0.6915055, 0.0047142757, 0.0021294942]\n",
            "Batch: 82\n",
            "D_Loss: [0.3185132 0.5      ]\n",
            "G_Loss: [0.97227985, 0.20961095, 0.69151103, 0.0033389125, 0.0037768742]\n",
            "Batch: 83\n",
            "D_Loss: [0.3185586 0.5      ]\n",
            "G_Loss: [0.9689688, 0.20949438, 0.6914676, 0.003396886, 0.0034037973]\n",
            "Batch: 84\n",
            "D_Loss: [0.31856245 0.5       ]\n",
            "G_Loss: [0.9459833, 0.20937638, 0.69140935, 0.0021576271, 0.0023621232]\n",
            "Batch: 85\n",
            "D_Loss: [0.31866142 0.5       ]\n",
            "G_Loss: [0.9611333, 0.20925361, 0.69142866, 0.0022081004, 0.0038370034]\n",
            "Batch: 86\n",
            "D_Loss: [0.31872913 0.5       ]\n",
            "G_Loss: [0.9411117, 0.20910999, 0.6913593, 0.0017661438, 0.0022980953]\n",
            "Batch: 87\n",
            "D_Loss: [0.31893957 0.5       ]\n",
            "G_Loss: [0.9455259, 0.20904765, 0.69144386, 0.0018526722, 0.0026507676]\n",
            "Batch: 88\n",
            "D_Loss: [0.3191027 0.5      ]\n",
            "G_Loss: [0.95376533, 0.20874941, 0.6914619, 0.0030427526, 0.0023126476]\n",
            "Batch: 89\n",
            "D_Loss: [0.31947434 0.5       ]\n",
            "G_Loss: [0.96294016, 0.20730393, 0.6915013, 0.003537469, 0.0028760177]\n",
            "Batch: 90\n",
            "D_Loss: [0.31853282 0.5       ]\n",
            "G_Loss: [0.975232, 0.20917122, 0.691508, 0.0023410823, 0.0051141977]\n",
            "Batch: 91\n",
            "D_Loss: [0.31850785 0.5       ]\n",
            "G_Loss: [0.9520759, 0.20927489, 0.6914187, 0.002268189, 0.0028700393]\n",
            "Batch: 92\n",
            "D_Loss: [0.3186167 0.5      ]\n",
            "G_Loss: [0.9612441, 0.20902246, 0.69133765, 0.0026962596, 0.0033921436]\n",
            "Batch: 93\n",
            "D_Loss: [0.318788 0.5     ]\n",
            "G_Loss: [0.9363236, 0.20852329, 0.69138294, 0.0019225918, 0.0017191464]\n",
            "Batch: 94\n",
            "D_Loss: [0.31877834 0.5       ]\n",
            "G_Loss: [0.95295054, 0.20908895, 0.6914597, 0.0018396075, 0.0034005735]\n",
            "Batch: 95\n",
            "D_Loss: [0.31879413 0.5       ]\n",
            "G_Loss: [0.9590236, 0.20895462, 0.6914352, 0.002074364, 0.0037890081]\n",
            "Batch: 96\n",
            "D_Loss: [0.318925 0.5     ]\n",
            "G_Loss: [0.94353765, 0.20842634, 0.69137174, 0.0020506794, 0.002323274]\n",
            "Batch: 97\n",
            "D_Loss: [0.3189249 0.5      ]\n",
            "G_Loss: [0.9538548, 0.20828539, 0.6913717, 0.0023089962, 0.0031107771]\n",
            "Batch: 98\n",
            "D_Loss: [0.31858382 0.5       ]\n",
            "G_Loss: [0.94892216, 0.20932102, 0.69139147, 0.0026676818, 0.0021532818]\n",
            "Batch: 99\n",
            "D_Loss: [0.318574 0.5     ]\n",
            "G_Loss: [0.9474277, 0.20930654, 0.6913869, 0.0023235292, 0.0023498954]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}